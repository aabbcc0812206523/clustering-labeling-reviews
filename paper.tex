\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Labeling review clusters}
\author{Khaled Hossain}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Introduction}

This is a very informal writing to keep note and track of ongoing labeling review cluster thesis. Our main goal is clustering of amazon product based on product aspect then find label for each cluster that can summarize the cluster.

\section{Basics}

\subsection{Dataset}

The dataset contains products reviews form amazon including 142 million reviews. We are considering electronics categories which contains 1,689,188 reviews. We manually annotated 300 reviews for three products.

\section{Labeling}

\subsection{Normalize clusters}

To apply any labeling approach, first we normalize all of our clusters by lower casing and removing stop words.

\subsection{Centroid label}

In this approach, we get term vector using word2vec. Document vector of each document in a cluster is calculated by applying vector addition of each term vector.
$$\text{DocumentVector} = \sum_{i=1}^{n} TermVector_i$$
Where $n$ is number of valid word2vec terms appeared in the document.

After calculating document vector of each documents in a cluster, we calculate center of the cluster by element wise

$$\text{ClusterCenter} = (\frac{\sum v[0]}{n}, \frac{\sum v[1]}{n}, \cdots, \frac{\sum v[i]}{n})$$

Then we calculate distance from every document vector to its centroid to find the minimum distance. The document that contains minimum distance from centroid is our label of the cluster.

$$\text{distance} = \sum_{i=1}^{n} (x_i-y_i)^2 $$

\subsection{Example result using above centroid labels}

none : If you go through headphones like I do then look no further.\\
price : These are great earbuds with a decent sound for the price.\\
sound quality : These are great earbuds with a decent sound for the price.\\
quality : These by far are the best headphones I have ever had.\\
noise cancellation : These earphones block out noise pretty well, and have very good audio quality.\\
cord : The cable jacket feels like PVC, which I prefer.\\
price worthy : Overall, I think they're worth the price.\\
comfortability : comfortable to wear and they do not hurt your ears.\\
design : Also the ergo fit design really works, the headphones haven't fallen out of my ears once which is awesome.\\
durability : I don't think these will last very long under heavy use.\\
fit : Likewise, if you don't like the whole sound, you can adjust the fitting in your ears.\\
color : I like the color and the build, I wish it has a built in mic tho.\\

\section{TF-IDF based centroid label}

In this approach, we calculate tf-idf value for each term in a cluster. Then, we add all term weight for each sentence in a cluster. The sentence contains height weight is selected as label.

$$SentenceWeight = \frac{1}{n} \sum_{i=1}^{n} TermWeight_i $$

In combined multigram model, we calculate sentence weight for unigram, bigram and trigram tokens, then add all three weight together.

\subsection{Example of unigram centroid labels}

none : You don't want them.\\
price : Fantastic sound at a great price.\\
sound quality : Good sound\\
quality : These earphones have to be one of the best earphones I have bought.\\
noise cancellation : These panasonic keep out exterior noise pretty well, too.\\
cord : The cord is thin.\\
price worthy : Worth the money, Thanks.\\
comfortability : comfortable to wear and they do not hurt your ears.\\
design : But with this unique ergo design, it addresses that problem.\\
durability : And I Love the "L" shaped haedphone jack, which seems more durable than the traditional straight-ilne jack.\\
fit : They fit great.\\
color : Plus comes in a nice variety of colors.\\

\subsection{Example of combined multigram centroid labels}

none : You don't want them.\\
price : Fantastic sound at a great price.\\
sound quality : Good sound\\
quality : These by far are the best headphones I have ever had.\\
noise cancellation : These panasonic keep out exterior noise pretty well, too.\\
cord : Another plus for me is that the cord splits and two equal lengths lead to each ear piece, as opposed to a short cord for one side and a longer cord for the other side after the split.\\
price worthy : Worth the money, Thanks.\\
comfortability : comfortable to wear and they do not hurt your ears.\\
design : But with this unique ergo design, it addresses that problem.\\
durability : And I Love the "L" shaped haedphone jack, which seems more durable than the traditional straight-ilne jack.\\
fit : Sound good and fit comfortably.\\
color : Plus comes in a nice variety of colors.\\


\section{Vector space model}

\subsection{Input clusters format}

The format of our input clusters is, clusters contains list of single cluster and a single cluster is a collection of documents

$$Clusters = [Cluster_1, Cluster_2, ... Cluster_n]$$
where n is number of cluster

$$Cluster = [Document_1, Document_2, ... Document_n]$$
where n is number of document in the cluster

\subsection{Vectorization}

We need to somehow convert our documents/clusters into some numbers to present them on vector space. To do that first we need to extract features from documents so that we can represent our documents by features vector

\subsection{Indexing Vocabulary}

For indexing vocabulary, we use all of our document in every cluster. So all documents will look like:

$$AllDocuments = [Document_1, Document_2, ... Document_n]$$
where n is number of documents of all clusters

Then we create a vocabulary by tokenizing all documents.

$$Tokens = [token_1, token_2, ... token_n]$$
where n is number of tokens

With this fixed n length (where n is number of tokens) Tokens we can represent our documents. This number of tokens will be our number of features. When indexing tokens vocabulary we also calculate idf value of each token

$$IDFVector = [idfValue_1, idfValue_2, ... idfValue_n]$$
where n is size of vocabulary

\subsection{Transforming clusters}
We like to represent each cluster by feature vector on a vector space. Here we count term frequency of each term in a cluster and multiply with previously calculated idf value of the term. Thats how we can represent a cluster on a vector space with tfidf features. But only tfidf value as a features is not enough to represent semantical information. Thats why we use Word2Vec embedding. In word2Vec every word is represented by 300 dimensional vector. We find 300 dimensional vector for each term (in our vocabulary) and multiply with tfidf value. Now each feature is represented by 300 dimensional vector. Multiplying a vector with a scaler does not change it's angle, the changes happens on amplitude. The corresponding vector will be scale up or down on amplitude level.

$$Cluster = [tfIdf_1, tfIdf_2, ... tfIdf_n]$$
where n is size of vocabulary.

\subsection{Combine similar terms}

General text contains lots of similar words. Treating two words those has similar meaning can potentially hamper our tfidf value. To avoid this we need to combine similar words. To do this, first we can use Word2Vec embedding where each word is represented with 300 dimensional vector. We represent each terms of our vocabulary with Word2Vec 300 dimensional vector. And then apply Hierarchical Agglomerative Clustering on them with complete linkage and cosine as distance metric with a certain thresold value (0.5 on our case). Cosine distance is looking into angle of a vector and complete linkage will ensure every term in a cluster has similar meaning.\\

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{hac.png}
\caption{\label{fig:hac}Clustering of tokens with Hierarchical Agglomerative Clustering.}
\end{figure}

Actual list of tokens for clustering:\\
tokens = ['noise', 'sound', 'loud', 'cancel', 'canceling', 'cancellation', 'headphones', 'earbuds', 'earbud', 'ipod', 'cord', 'cable', 'cords', 'cables', 'jacket', 'cheap', 'price', 'color', 'plastic', 'tangle']\\

\textbf{Agglomerative Clustering of tokens:}\\
\textit{[cancel, canceling, cancellation]}, [cheap], [price], [noise, loud], [sound], [color], \textit{[cord, cords]}, [plastic], \textit{[headphones, earbuds, earbud]}, [ipod], [jacket], \textit{[cable, cables]}, [tangle]\\


For each cluster (represented as tfidf features), We find cluster of similar terms (vocabulary cluster) and pick top weighted tfidf term and sum tfidf value of other terms on same vocabulary cluster. Replace top weighted term with summed value and other terms(in same vocabulary cluster) with 0.

\subsection{Combined TfIdf and words embedding on features vector}

Only tfidf value as a features is not enough to represent semantical information. Thats why we can use Word2Vec embedding. In word2Vec every word is represented by 300 dimensional vector. We find 300 dimensional vector for each term (in our vocabulary) and multiply with tfidf value. Now each feature is represented by 300 dimensional vector. Multiplying a vector with a scaler does not change it's angle, the changes happens on amplitude. The corresponding vector will be scale up or down on amplitude level.

$$Cluster = [feature_1, feature_2, ... feature_n]$$
where each feature is 300 dimensional vector and n is size of vocabulary.

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}